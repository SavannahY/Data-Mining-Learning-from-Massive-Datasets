# -*- coding: utf-8 -*-
"""ese545project3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n6A7kX5_aYRjFSojX8B-hCWHFX4ybb6G
"""

from google.colab import drive
drive.mount('/content/gdrive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""# Module 1: movie recommendation system"""

path = 'gdrive/My Drive/ESE 545/proj3/recommendationMovie.csv'
df_org = pd.read_csv(path,header=None)

def evaluate(loss,optimal_loss):
    regret = loss-optimal_loss
    print('loss plot')
    plt.plot(np.arange(len(loss)),loss)
    plt.show()
    print('optimal loss plot')
    plt.plot(np.arange(len(loss)),optimal_loss)
    plt.show()
    print('regret plot')
    plt.plot(np.arange(len(loss)),regret)
    plt.show()
    print('regret/t plot')
    regret_dt = regret/(np.arange(len(loss))+1)
    plt.plot(np.arange(len(loss)),regret_dt)
    plt.show()

"""## partial feedback
### stochastic method
- ucb
- thompson sampling
"""

def ucb_partial(df_org):
    mu = np.zeros(len(df_org))
    n = np.zeros(len(df_org),dtype = int)
    arm = np.zeros(df_org.shape[1],dtype = int)
    loss = np.zeros(df_org.shape[1],dtype = int)
    optimal_loss = np.zeros(df_org.shape[1],dtype = int)

    # try all arms once
    for i in range(len(df_org)):
        arm[i] = i
        loss[i] = sum(1-mu[arm[:i+1]])
        optimal_loss[i] = (i+1)*(1-max(mu))
        n[i] += 1
        mu[i] += df_org.iloc[i,i]
    for t in range(df_org.shape[0],df_org.shape[1]):
        ucb = mu+np.sqrt(2*np.log(t)/n)
        idx = np.argmax(ucb)
        arm[t] = idx
        loss[t] = sum(1-mu[arm[:t+1]])
        optimal_loss[t] = (t+1)*(1-max(mu))
        r = df_org.iloc[idx,t]
        n[idx] += 1
        mu[idx] += 1/n[idx]*(r-mu[idx])
    return loss,optimal_loss

def Tsap_partial(df_org):
    loss = 0
    arm = np.zeros(df_org.shape[1],dtype = int)
    loss = np.zeros(df_org.shape[1],dtype = int)
    optimal_loss = np.zeros(df_org.shape[1],dtype = int)
    s = np.zeros(len(df_org))
    f = np.zeros(len(df_org))
    for t in range(df_org.shape[1]):
        mu = (s+1)/(s+f+2)
        samples = np.random.beta(s+1,f+1)
        idx = np.argmax(samples)
        arm[t] = idx
        loss[t] = sum(1-mu[arm[:t+1]])
        optimal_loss[t] = (t+1)*(1-max(mu))
        r = df_org.iloc[idx,t]
        if r:
            s[idx] += 1
        else:
            f[idx] += 1
    return loss,optimal_loss

loss,optimal_loss = ucb_partial(df_org)
evaluate(loss,optimal_loss)

loss,optimal_loss = Tsap_partial(df_org)
evaluate(loss,optimal_loss)

"""### non-stochastic method"""

def exp3_partial(df_org):
    pr = np.ones(df_org.shape[0])/df_org.shape[0]    # initialize distribution
    L_hat = np.zeros(df_org.shape[0])
    movie_losses_over_time = np.zeros(df_org.shape[0])
    loss = np.zeros(df_org.shape[1],dtype=int)
    optimal_loss = np.zeros(df_org.shape[1],dtype=int)
    for t in range(df_org.shape[1]):
        movie_losses_over_time += 1 - df_org.iloc[:,t]
        idx = np.random.choice(np.arange(df_org.shape[0]), p=pr)
        l_hat = (1-df_org.iloc[idx,t])/pr[idx]
        L_hat[idx] += l_hat 
        yita = np.sqrt(np.log(df_org.shape[0])/((t+1)*df_org.shape[0]))
        pr = np.exp(-yita*L_hat)
        pr = pr/sum(pr)
        loss[t] = loss[t-1] + (1 - df_org.iloc[idx,t])
        optimal_loss[t] = min(movie_losses_over_time)
    return loss,optimal_loss

loss,optimal_loss = exp3_partial(df_org)
evaluate(loss,optimal_loss)

"""## full feedback
### stochastic method
"""

def ucb_full(df_org):
    mu = np.zeros(len(df_org))
    n = np.zeros(len(df_org),dtype = int)
    arm = np.zeros(df_org.shape[1],dtype=int)
    loss = np.zeros(df_org.shape[1],dtype = int)
    optimal_loss = np.zeros(df_org.shape[1],dtype = int)

    # try all arms once
    for i in range(len(df_org)):
        arm[i] = i
        loss[i] = sum(1-mu[arm[:i+1]])
        optimal_loss[i] = (i+1)*(1-max(mu))
        n[i] += 1
        mu += 1/(i+1)*(df_org.iloc[:,i]-mu)
    for t in range(df_org.shape[0],df_org.shape[1]):
        ucb = mu+np.sqrt(2*np.log(t)/n)
        idx = np.argmax(ucb)
        n[idx] += 1
        arm[t] = idx
        loss[t] = sum(1-mu[arm[:t+1]])
        optimal_loss[t] = (t+1)*(1-max(mu))
        mu += 1/(t+1)*(df_org.iloc[:,t]-mu)
    return loss,optimal_loss

loss,optimal_loss = ucb_full(df_org)
evaluate(loss,optimal_loss)

def Tsap_full(df_org):
    loss = np.zeros(df_org.shape[1],dtype = int)
    arm = np.zeros(df_org.shape[1],dtype = int)
    optimal_loss = np.zeros(df_org.shape[1],dtype = int)
    s = np.zeros(len(df_org))
    f = np.zeros(len(df_org))
    for t in range(df_org.shape[1]):
        mu = (s+1)/(s+f+2)
        samples = np.random.beta(s+1,f+1)
        idx = np.argmax(samples)
        arm[t] = idx
        loss[t] = sum(1-mu[arm[:t+1]])
        optimal_loss[t] = (t+1)*(1-max(mu))
        s[df_org.iloc[:,t]==1] += 1
        f[df_org.iloc[:,t]==0] += 1
    return loss,optimal_loss

loss,optimal_loss = Tsap_full(df_org)
evaluate(loss,optimal_loss)

"""### non-stochastic method"""

def mulweights(df_org,best_movies = None):
    T = df_org.shape[1]
    K = df_org.shape[0]
    yita = 1/np.sqrt(T)
    weight = np.ones(K)
    loss = np.zeros(df_org.shape[1],dtype = int)
    optimal_loss = np.zeros(df_org.shape[1],dtype = int)
    movie_losses_over_time = np.zeros(df_org.shape[0])
    pr_best_movies = np.zeros((10,T))
    for t in range(T):
        movie_losses_over_time += 1 - df_org.iloc[:,t]
        pr = weight/sum(weight)
        if best_movies!=None:
            pr_best_movies[:,t] = pr[best_movies] 
        idx = np.random.choice(np.arange(df_org.shape[0]), p=pr)
        weight = weight*(1 - yita*(1-df_org.iloc[:,t]))
        loss[t] = loss[t-1] + (1 - df_org.iloc[idx,t])
        optimal_loss[t] = min(movie_losses_over_time)
    return loss,optimal_loss,pr_best_movies

loss,optimal_loss,_ = mulweights(df_org)
evaluate(loss,optimal_loss)

"""## movie analysis

analyze how the recommendation probabilities for movies changes over time.
"""

movies = df_org.sum(1).to_numpy()
best_movies = []
for i in range(10):
    best_movie = np.argmax(movies)
    best_movies.append(best_movie)
    movies = np.delete(movies,best_movie)

_,pr_best_movies = mulweights(df_org,best_movies)

fig,ax = plt.subplots(figsize = (10,7))
for i in range(10):
    ax.plot(np.arange(pr_best_movies.shape[1]),pr_best_movies[i,:],label='movie '+str(best_movies[i]))
ax.set_xlabel('time')
ax.set_ylabel('probability')
ax.set_yscale("log")
ax.legend()
plt.show()

fig,ax = plt.subplots(figsize = (10,7))
for i in range(10):
    ax.plot(np.arange(pr_best_movies.shape[1]),pr_best_movies[i,:],label='movie '+str(best_movies[i]))
ax.set_xlabel('time')
ax.set_ylabel('probability')
ax.set_yscale("log")
ax.legend()
ax.set_yscale("linear")
ax.set_title('movie recommendation probability over time in linear scale')
plt.show()

"""# Module 2: Clusting"""

path = 'gdrive/My Drive/ESE 545/proj3/Movies.csv'
df = pd.read_csv(path,header=None)

"""## data preprocessing
- add features:
  - year
  - movie title type
  - movie length
"""

# decide how to choose the movie length features
movie_years = df.iloc[1:,4].astype(int).to_numpy()
fig, axs = plt.subplots(figsize = (10,7))
axs.hist(movie_years,100)
axs.set_title('data distribution of movie year')
plt.show()

def check_year(start_year,check_year_min,check_year_max):
    if start_year=='startYear':
        return check_year_min
    return int(int(start_year)>=check_year_min and int(start_year)<check_year_max)

years = [0,1950,1980,2000,2005,2010,2015,2020,9999]
for i in range(len(years)-1):
    df[i+34] = df[4].apply(check_year,args = (years[i],years[i+1]))

titleType = list(dict.fromkeys(df.iloc[1:,1]))

def check_title(title,check_title,_):
    if title=='titleType':
        return check_title
    return int(title==check_title)

for idx in range(len(titleType)):
    df[42+idx] = df[1].apply(check_title,args = (titleType[idx],titleType[idx]))

# decide how to choose the movie length features
runtimemin = df.iloc[1:,5].astype(int)
runtime = runtimemin.to_numpy()
rumtime = runtime[runtime<150]
fig, axs = plt.subplots(figsize = (10,7))
axs.hist(runtime,100)
axs.set_title('data distribution of movie run time')
plt.show()

def check_length(length,_min_,_max_):
    if length=='runtimeMinutes':
        return _min_
    return int(int(length)>_min_ and int(length)<_max_)

length_bd = [0,18,35,50,70,110,130,np.inf] 
for i in range(len(length_bd)-1):
    df[52+i] = df[5].apply(check_length,args = (length_bd[i],length_bd[i+1]))

df = df.drop([0,1,2,4,5],1)
# save final dataframe
pkl_path = 'gdrive/My Drive/ESE 545/proj3/df_final.pkl'
df.to_pickle(pkl_path)

# read final dataframe
df = pd.read_pickle(pkl_path)

data = df.drop(0,0).to_numpy(dtype=int)

"""## mini-batch kmeans
- random initialization
"""

T = 100000
N = data.shape[0]
p = data.shape[1]
K = [5,10,15,20,50,100,300,500]
label = np.zeros(N,dtype=int)
min_dis = []
max_dis = []
mean_dis = []
for k in K:
    centers = np.random.uniform(0,1,size=(k,p))
    for t in range(T):
        yita = 1/(t+1)
        x_rand = data[np.random.randint(0,N)]
        center_idx = np.argmin(np.linalg.norm(centers - x_rand,axis=1))
        centers[center_idx] += yita*(x_rand-centers[center_idx])
    for i in range(N):
        label[i] = np.argmin(np.linalg.norm(centers - data[i],axis=1))
    distance = np.linalg.norm(data-centers[label],axis=1)
    min_dis.append(min(distance))
    max_dis.append(max(distance))
    mean_dis.append(np.mean(distance))

print('min distance')
fig,ax = plt.subplots()
ax.plot(K,min_dis)
ax.set_xlabel('# of centroids')
ax.set_ylabel('distance')
plt.show()

print('max distance')
fig,ax = plt.subplots()
ax.plot(K,max_dis)
# ax.set_ylim(2.65,2.75)
ax.set_xlabel('# of centroids')
ax.set_ylabel('distance')
plt.show()

print('mean distance')
fig,ax = plt.subplots()
ax.plot(K,mean_dis)

ax.set_xlabel('# of centroids')
ax.set_ylabel('distance')
plt.show()

"""## kmeans++"""

T = 10000
N = data.shape[0]
p = data.shape[1]
K = [5,10,15,20,50,100,150,200]
label = np.zeros(N,dtype=int)
min_dis = []
max_dis = []
mean_dis = []
for k in K:
    # initialization
    centers = np.random.uniform(0,1,size=(k,p))
    centers[0] = data[np.random.randint(0,N)]
    for k_i in range(1,k):
        pr = np.zeros(N)
        for i in range(N):
            pr[i] = np.min(np.linalg.norm(centers-data[i],axis=1))**2
        pr = pr/sum(pr)
        centers[k_i] = data[np.random.choice(np.arange(N),p=pr)]
    # same as sgd kmeans
    for t in range(T):
        yita = 1/(t+1)
        x_rand = data[np.random.randint(0,N)]
        center_idx = np.argmin(np.linalg.norm(centers - x_rand,axis=1))
        centers[center_idx] += yita*(x_rand-centers[center_idx])
    for i in range(N):
        label[i] = np.argmin(np.linalg.norm(centers - data[i],axis=1))
    distance = np.linalg.norm(data-centers[label],axis=1)
    min_dis.append(min(distance))
    max_dis.append(max(distance))
    mean_dis.append(np.mean(distance))

print('min distance')
fig,ax = plt.subplots()
ax.plot(K,min_dis)
ax.set_xlabel('# of centroids')
ax.set_ylabel('distance')
plt.show()

print('max distance')
fig,ax = plt.subplots()
ax.plot(K,max_dis)
ax.set_xlabel('# of centroids')
ax.set_ylabel('distance')
plt.show()

print('mean distance')
fig,ax = plt.subplots()
ax.plot(K,mean_dis)
ax.set_ylim(1.75,2.25)
ax.set_xlabel('# of centroids')
ax.set_ylabel('distance')
plt.show()

"""## reference results from scikit learn"""

from sklearn.cluster import MiniBatchKMeans, KMeans 
from sklearn.metrics.pairwise import pairwise_distances_argmin 
from sklearn.datasets.samples_generator import make_blobs 
  
# Load data in X  
batch_size = 100
K = [5,10,15,20,50,100,300,500]
min_dis = []
max_dis = []
mean_dis = []
# perform the mini batch K-means 
for k in K:
    mbk = MiniBatchKMeans(init ='k-means++', n_clusters = k, max_iter=10000 ,
                        batch_size = batch_size, n_init = 10, 
                        max_no_improvement = 10, verbose = 0) 
    
    mbk.fit(data) 
    mbk_means_cluster_centers = np.sort(mbk.cluster_centers_, axis = 0) 
    mbk_means_labels = pairwise_distances_argmin(data, mbk_means_cluster_centers) 
    distance = np.linalg.norm(data-mbk_means_cluster_centers[mbk_means_labels],axis=1)
    min_dis.append(min(distance))
    max_dis.append(max(distance))
    mean_dis.append(np.mean(distance))
# print the labels of each data

print('min distance')
fig,ax = plt.subplots()
ax.plot(K,min_dis)
ax.set_xlabel('# of centroids')
ax.set_ylabel('distance')
plt.show()

print('max distance')
fig,ax = plt.subplots()
ax.plot(K,max_dis)
ax.set_xlabel('# of centroids')
ax.set_ylabel('distance')
plt.show()

print('mean distance')
fig,ax = plt.subplots()
ax.plot(K,mean_dis)
ax.set_ylim(1.75,2.25)
ax.set_xlabel('# of centroids')
ax.set_ylabel('distance')
plt.show()

